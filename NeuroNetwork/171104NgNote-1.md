> Andrew Ng's course "Neural networks deep learning" notes

## 01 Introduction to deep learning

在 AI 的各个分支中，发展最为迅速的就是深度学习。

深度学习指的是训练神经网络。

### 神经网络 Neural Network

什么是神经网络？

举个例子，我们要预测房价的走势。如果知道房子大小我们可以预测房价，这个关系就可以用一个神经网络节点（node）来简单估计。


![](http://7xjpra.com1.z0.glb.clouddn.com/ngcourseneuro.png)


如果我们知道房子的信息很多的时候怎么办呢？这时候就需要很多的节点，这些节点构成神经网络。房子的多种信息作为输入，房价的预测值作为输出，中间层（可以有多个）是用来计算出前面一层信息的权重，得出一定的模式，传导给下一层，直到最后得出预测值 y。这些中间层可以理解为神经网络。

![](http://7xjpra.com1.z0.glb.clouddn.com/ngcourse_housingprice.png)


### 监督学习 Supervised Learning


监督学习中，你有一些输入 x，想将 x 映射到输出 y。比如说有一个预测房价的应用软件，它可以根据你输入的房屋特征 x， 去估计房屋的售价 y。在监督学习中，我们拿到的训练数据是标签好的。

很多有价值的发明，都是通过神经网络在特定问题下来巧妙地建立 x 对应 y 的函数映射关系，并且通过监督学习拟合数据，成为某个复杂系统的一部分。

在图像应用中我们常常将卷积结构放在神经网络结构当中，简称为 CNN。而在序列化数据比如音频、语言中，常常用到 RNN，即循环神经网络。

在神经网络崛起后，最令人兴奋的事情之一就是它所带来的深度学习使得计算机能够比前些年更好地解释非结构化数据，以实现许多新的应用，例如语音识别、图像识别、在文本上的自然语言处理技术。

### 深度学习的兴起

三起三落的神经网络

![](http://images2015.cnblogs.com/blog/673793/201512/673793-20151228170208120-1856567090.jpg)

![](http://images2015.cnblogs.com/blog/673793/201512/673793-20151228134016120-1091351096.jpg)

图片 via [神经网络浅讲：从神经元到深度学习](http://www.cnblogs.com/subconscious/p/5058741.html#fifth)


深度学习背后的基本思想已经出现了好几十年了，为什么最近才兴起呢？

![](https://github.com/XingxingHuang/deeplearning.ai/raw/master/1_Neural%20Networks%20and%20Deep%20Learning/week1/img/1.3.png)

上面这个图，纵坐标代表性能，横坐标代表数据量。

过去的20年左右，对于很多问题，需要处理的数据量都从一个相对小的数据集变成了一个非常大的数据集。如果我们训练一个特别大的神经网络，它的性能通常会变得越来越好。深度学习的初期崛起，得益于数据的规模以及计算的规模。


## 02 Neural Networks Basics


### 1. logistic-regression-as-a-neural-network 逻辑回归

#### Binary classification 二分类问题

逻辑回归是处理二分类问题的一种算法。什么是分类问题呢？比如判断一张图片里有没有喵星人：


在二元分类问题中，目标是构建一个分类器： 输入一幅以特征向量 x 表示的图像，然后预测对应的输出 y 是1还是0，即这幅图上是猫还是不是猫。

单个样本由一对（x,y）表示 其中，x 是一个 $n_x$ 维的特征向量 y是标签，取值为0或1。训练集包含m个训练样本。

矩阵 X 的大小 X.shape = $(n_x, m)$，矩阵 Y 的大小 Y.shape = $(1, m)$

线性回归：

$$ \hat{y} = w^T + b $$

这并不适合二分类问题，因为我们想让 y 在[0,1]之间，但是线性回归得到的 y 可能远大于这个范围。所以我们使用 sigmoid (σ) 函数

$$ \sigma(z) = \frac {1}{1 + e^{-z}} $$

$$ \hat{y} = \sigma{( w^T + b )}$$

#### Cost function 损失函数

损失函数用于计算样本输出值与目标值之间的差值，差值越大，说明训练「效果」越差，「损失」越大，这将帮助我们=评估分类的准确性。对比刻意练习的原则，算法「练习」的目标，就是不断降低损失函数以逼近目标值，而损失函数也是每一步训练必不可少的「反馈」。最简单的损失函数：

$$ L(\hat{y},y) = 1/2 (\hat{y} - y)^2 $$

在逻辑回归中，我们不用平方差做为损失函数，而是

$$ J(w,b) = \frac{1}{m}\sum^m_{i=1} L(\hat{y}^{(i)},y^{(i)}) =  -\frac{1}{m}\sum^m_{i=1} [(y^{(i)}log(\hat{y}^{(i)}) + (1-y^{(i)})log(1- \hat{y}^{(i)})]$$

推导如下：

![](http://7xjpra.com1.z0.glb.clouddn.com/vlcsnap-2017-11-06-15h48m50s923.png)

我们要做的，就是找出令损失函数值最小的 w 和 b 的取值。

### 02_python-and-vectorization





### Ref

- [XingxingHuang/deeplearning.ai: The notes and answers of Adrew Ng's coursera course: deeplearning.ai](https://github.com/XingxingHuang/deeplearning.ai)
- [marsggbo/deeplearning.ai_JupyterNotebooks: DeepLearning.ai课程学习Jupyter Notebook作业](https://github.com/marsggbo/deeplearning.ai_JupyterNotebooks)
